{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "from math import sqrt, log, exp\n",
    "import pandas as pd\n",
    "import string\n",
    "\n",
    "# dictionary = pd.read_csv(\"Assignment10-WordFrequencies.csv\", header=None)\n",
    "# dictionary.insert(2, \"P(word)\", dictionary[1]/word_freq_sum)\n",
    "# dictionary.to_csv(\"P_word.csv\", index=False)\n",
    "\n",
    "dictionary = pd.read_csv(\"P_word.csv\", header=None)\n",
    "word_freq_sum = np.sum(dictionary[1])\n",
    "vowels = ['a', 'e', 'i', 'o', 'u']\n",
    "alphabets = list(string.ascii_lowercase) #alphabet letters in list\n",
    "NUM_WORDS = dictionary.shape[0]\n",
    "\n",
    "# #Q1\n",
    "word_prob_list = [float(each) for each in dictionary[2][1:]] #reads each word's probability into a list\n",
    "entropy = sum([word_p * log(1/word_p, 2) for word_p in word_prob_list])\n",
    "print(\"Entropy over words in English: \", entropy)\n",
    "\n",
    "#Q2\n",
    "word_prob_code_len_pair = [(word_p, log(1/word_p, 2)) for word_p in word_prob_list] #Creates a pair of word probability and its bits length according to frequency\n",
    "entropy_by_freq = entropy\n",
    "\n",
    "uni_prob = 1 / NUM_WORDS\n",
    "uni_prob_code_len_pair = [(uni_prob, log(1/uni_prob, 2)) for _ in range(NUM_WORDS)] #Creates a pair of word probability and its bits length according to uniform probability\n",
    "entropy_by_uni = sum([uni_prob * length for uni_prob, length in uni_prob_code_len_pair])\n",
    "\n",
    "#Computes the win_rate for words guessed for the pair\n",
    "def win_rate(pair):\n",
    "    win_p = 0\n",
    "    for word_p, code_length in pair:\n",
    "        if code_length <= 20:\n",
    "            win_p += word_p\n",
    "    return win_p\n",
    "\n",
    "print(\"2(a)- Win Rate for words chosen by frequencyy: {0}, Entropy:{1} \".format(round(win_rate(word_prob_code_len_pair), 5), round(entropy_by_freq, 5)))\n",
    "\n",
    "print(\"2(b)- Win Rate for words chosen by uniformity: {0}, Entropy: {1} \".format(round(win_rate(uni_prob_code_len_pair), 5), round(entropy_by_uni, 5)))\n",
    "\n",
    "\n",
    "#Q3\n",
    "letter_prob_count, word_count = {each: 0 for each in alphabets}, {each: 0 for each in alphabets}\n",
    "dictionary[0] = dictionary[0].astype(str)\n",
    "\n",
    "#Finds frequencies of all the words that start with each character\n",
    "def first_letter_count():\n",
    "    for i in range(1, dictionary.shape[0]):\n",
    "        first_letter = dictionary.iloc[i, 0].lower()[0]\n",
    "        word_count[first_letter] += dictionary.iloc[i, 1]\n",
    "\n",
    "#Finds conditional entropy for each character \n",
    "def word_prob():\n",
    "    for alphabet in word_count:\n",
    "        total = word_count[alphabet]\n",
    "        for i in range(1, dictionary.shape[0]):\n",
    "            freq = dictionary.iloc[i, 1]\n",
    "            first_letter = dictionary.iloc[i, 0].lower()[0]\n",
    "            prob = freq / total\n",
    "            if alphabet == first_letter:\n",
    "                letter_prob_count[alphabet] += prob * log(1/prob, 2)\n",
    "\n",
    "first_letter_count()\n",
    "word_prob()\n",
    "x = letter_prob_count.keys()\n",
    "y = letter_prob_count.values()\n",
    "\n",
    "plt.bar(x, y, color='orangered')\n",
    "plt.xlabel(\"First character\")\n",
    "plt.ylabel(\"Conditional Entropy\")\n",
    "plt.title('Conditional Entropies by Conditioning the First Character')\n",
    "\n",
    "plt.savefig(\"3.png\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#Q4\n",
    "\n",
    "\n",
    "letter_prob_count, word_count = {each: 0 for each in alphabets}, {each: 0 for each in alphabets}\n",
    "dictionary[0] = dictionary[0].astype(str)\n",
    "\n",
    "\n",
    "# Finds frequencies of all the words that start with each character\n",
    "def first_letter_count():\n",
    "    for i in range(1, dictionary.shape[0]):\n",
    "        first_letter = dictionary.iloc[i, 0].lower()[0]\n",
    "        word_count[first_letter] += dictionary.iloc[i, 1]\n",
    "\n",
    "\n",
    "# Finds conditional entropy for each character\n",
    "def word_prob():\n",
    "    for alphabet in word_count:\n",
    "        total = word_count[alphabet]\n",
    "        for i in range(1, dictionary.shape[0]):\n",
    "            freq = dictionary.iloc[i, 1]\n",
    "            first_letter = dictionary.iloc[i, 0].lower()[0]\n",
    "            prob = freq / total\n",
    "            if alphabet == first_letter:\n",
    "                letter_prob_count[alphabet] += prob * log(1 / prob, 2)\n",
    "        letter_prob_count[alphabet] = total / word_freq_sum * letter_prob_count[alphabet]\n",
    "\n",
    "first_letter_count()\n",
    "word_prob()\n",
    "\n",
    "cond_entropy = letter_prob_count.values()\n",
    "mutual_first = entropy - sum(cond_entropy)\n",
    "\n",
    "\n",
    "last_letter_prob_count, last_count, vowel_prob_count, vowel_count = {each: 0 for each in alphabets}, {each: 0 for each in alphabets}, {\n",
    "each: 0 for each in vowels}, {each: 0 for each in vowels}\n",
    "dictionary[0] = dictionary[0].astype(str)\n",
    "\n",
    "\n",
    "# Finds frequencies of all the words that start with each character\n",
    "def last_letter_count():\n",
    "    for i in range(1, dictionary.shape[0]):\n",
    "        last_letter = dictionary.iloc[i, 0].lower()[-1]\n",
    "        last_count[last_letter] += dictionary.iloc[i, 1]\n",
    "\n",
    "\n",
    "# Finds conditional entropy for each last character\n",
    "def last_letter_prob():\n",
    "    for alphabet in last_count:\n",
    "        total = last_count[alphabet]\n",
    "        for i in range(1, dictionary.shape[0]):\n",
    "            freq = dictionary.iloc[i, 1]\n",
    "            last_letter = dictionary.iloc[i, 0].lower()[-1]\n",
    "            prob = freq / total\n",
    "            if alphabet == last_letter:\n",
    "                last_letter_prob_count[alphabet] += prob * log(1 / prob, 2)\n",
    "        last_letter_prob_count[alphabet] = total / word_freq_sum * last_letter_prob_count[alphabet]\n",
    "\n",
    "\n",
    "last_letter_count()\n",
    "last_letter_prob()\n",
    "\n",
    "cond_entropy = last_letter_prob_count.values()\n",
    "mutual_last = entropy - sum(cond_entropy)\n",
    "\n",
    "def vowel_letter_count():\n",
    "    for i in range(1, dictionary.shape[0]):\n",
    "        word = dictionary.iloc[i, 0].lower()\n",
    "        index = 0\n",
    "        while index < len(word):\n",
    "            if word[index] in vowels:\n",
    "                vowel_count[word[index]] += dictionary.iloc[i, 1]\n",
    "                break\n",
    "            index += 1\n",
    "\n",
    "\n",
    "# Finds conditional entropy for each first vowel\n",
    "def vowel_letter_prob():\n",
    "    global vowel_prob_count\n",
    "    for i in range(1, dictionary.shape[0]):\n",
    "        freq = dictionary.iloc[i, 1]\n",
    "        word = dictionary.iloc[i, 0]\n",
    "        index = 0\n",
    "        while index < len(word):\n",
    "            if word[index] in vowels:\n",
    "                total = vowel_count[word[index]]\n",
    "                prob = freq / total\n",
    "                vowel_prob_count[word[index]] += prob * log(1/prob, 2)\n",
    "                break\n",
    "            index += 1\n",
    "    vowel_prob_count = {vowel:vowel_count[vowel]/sum(vowel_count.values()) * vowel_prob_count[vowel] for vowel in vowel_prob_count}\n",
    "\n",
    "\n",
    "# vowel_letter_count()\n",
    "# vowel_letter_prob()\n",
    "\n",
    "# cond_entropy = vowel_prob_count.values()\n",
    "# mutual_vowel = entropy - sum(cond_entropy)\n",
    "\n",
    "# x = ['First Letter', 'Last Letter', 'First Vowel']\n",
    "# y = [mutual_first, mutual_last, mutual_vowel]\n",
    "\n",
    "# plt.bar(x, y, color='orangered')\n",
    "# plt.xlabel(\"Given Data\")\n",
    "# plt.ylabel(\"Information (Entropy - Conditional Entropy)\")\n",
    "# plt.title('Information given about word identity conveyed by first letter, last letter, vowels')\n",
    "\n",
    "# plt.savefig(\"4c.png\")\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# Q5\n",
    "length_dict = {}\n",
    "dictionary[0] = dictionary[0].astype(str)\n",
    "def letter_length_count():\n",
    "    for i in range(1, dictionary.shape[0]):\n",
    "        word_length = str(len(dictionary.iloc[i, 0]))\n",
    "        if word_length in length_dict:\n",
    "            length_dict[word_length] += dictionary.iloc[i, 1]\n",
    "        else:\n",
    "            length_dict[word_length] = dictionary.iloc[i, 1]\n",
    "\n",
    "letter_length_count()\n",
    "sorted_len_dict = {length: length_dict[length] for length in sorted(length_dict, key=lambda x:int(x))} #Sorts the length_dict in increasing length and creates new dictionary of it\n",
    "length_prob = {key: 0 for key in sorted_len_dict.keys()}\n",
    "\n",
    "\n",
    "unique_word_count = {each:0 for each in sorted_len_dict.keys()}\n",
    "\n",
    "def length_letter_prob():\n",
    "    for i in range(1, dictionary.shape[0]):\n",
    "        length = len(dictionary.iloc[i, 0])\n",
    "        total = word_freq_sum\n",
    "        freq = dictionary.iloc[i, 1]\n",
    "        prob = freq / total\n",
    "        length_prob[str(length)] += log(1/prob, 2)\n",
    "        unique_word_count[str(length)] += 1\n",
    "\n",
    "length_letter_prob()\n",
    "length_prob = {length: length_prob[length] / sorted_len_dict[length] for length in length_prob}\n",
    "\n",
    "x = [int(length) for length in length_prob.keys()]\n",
    "y = length_prob.values()\n",
    "\n",
    "plt.bar(x, y, color='orangered')\n",
    "plt.xlabel(\"Word Length\")\n",
    "plt.ylabel(\"Average surprisal\")\n",
    "plt.title('Average Surprisal based on the word length')\n",
    "\n",
    "plt.savefig(\"5.png\")\n",
    "plt.show()\n",
    "\n",
    "# Q6\n",
    "MEASURE_CONST = sum([y/x for y, x in list(zip(y, x))]) / len(x) #This is a measure constant that evaluates how well the avg surprisal agrees with the word length. How I find the measure is by finding the ratio between avg surprisal & number of word length for each possible length found in #5 and then finding their average, so that this gives a correlation constant for average surprisal and word length. I will do the same for N=10, N=20, N=30... and divide each measure by this constant to find the measure. If it is greater than 1, that means it has higher measure, which can be interpreted as having stronger agreement between average surprisal and word length.\n",
    "\n",
    "dictionary[0] = dictionary[0].astype(str)\n",
    "variance_list = []\n",
    "\n",
    "def surprisal_finder():\n",
    "    N = 10\n",
    "    N_MAX = 6000\n",
    "\n",
    "    while N <= N_MAX:\n",
    "        length_dict = {}\n",
    "\n",
    "        for i in range(1, N):\n",
    "            word_length = str(len(dictionary.iloc[i, 0]))\n",
    "            if word_length in length_dict:\n",
    "                length_dict[word_length] += dictionary.iloc[i, 1]\n",
    "            else:\n",
    "                length_dict[word_length] = dictionary.iloc[i, 1]\n",
    "\n",
    "        sorted_len_dict = {length: length_dict[length] for length in sorted(length_dict, key=lambda x: int(x))}\n",
    "        length_prob = {key: 0 for key in sorted_len_dict.keys()}\n",
    "        unique_word_count = {each: 0 for each in sorted_len_dict.keys()}\n",
    "\n",
    "        for i in range(1, N):\n",
    "            length = len(dictionary.iloc[i, 0])\n",
    "            total = word_freq_sum\n",
    "            freq = dictionary.iloc[i, 1]\n",
    "            prob = freq / total\n",
    "            length_prob[str(length)] += log(1 / prob, 2)\n",
    "            unique_word_count[str(length)] += 1\n",
    "\n",
    "        length_prob = {length: length_prob[length] / unique_word_count[length] for length in length_prob}\n",
    "        x = [int(length) for length in length_prob.keys()]\n",
    "        y = length_prob.values()\n",
    "        measured_const_val = sum([y / x for y, x in list(zip(y, x))]) / len(x)\n",
    "        variance_list.append(measured_const_val/MEASURE_CONST)\n",
    "        N += 10\n",
    "        print(N)\n",
    "\n",
    "surprisal_finder()\n",
    "\n",
    "x = [10 * i for i in range(1, len(variance_list)+1)]\n",
    "y = variance_list\n",
    "\n",
    "plt.plot(x, y, color='orangered')\n",
    "plt.xlabel(\"N (Most frequent N words)\")\n",
    "plt.ylabel(\"Surprisal Variance(measured avg ratio / measure const\")\n",
    "plt.title('Correlation between number of most frequent words and agreement of surprisal (y >= 1.0 : strong / y <= 1.0: weak')\n",
    "\n",
    "plt.savefig(\"6.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
